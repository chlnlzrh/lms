{
  "requests": [
    {
      "custom_id": "lesson_M0-L001",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M0-L001\n- Lesson Title: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence)\n- Module: M0 ‚Äî AI-Native Data Visualization Foundations\n- Subtopic / Focus Area: General\n- Complexity Level: F (Foundation)\n- Estimated Duration: 45 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M0-L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence)` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M0-L002",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M0-L002\n- Lesson Title: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM\n- Module: M0 ‚Äî AI-Native Data Visualization Foundations\n- Subtopic / Focus Area: General\n- Complexity Level: F (Foundation)\n- Estimated Duration: 45 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M0-L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M0-L003",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M0-L003\n- Lesson Title: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps)\n- Module: M0 ‚Äî AI-Native Data Visualization Foundations\n- Subtopic / Focus Area: General\n- Complexity Level: F (Foundation)\n- Estimated Duration: 45 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M0-L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps)` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M0-L004",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M0-L004\n- Lesson Title: RAG for Viz: pulling metric definitions/policies/MBR decks with citations\n- Module: M0 ‚Äî AI-Native Data Visualization Foundations\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M0-L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M0-L005",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M0-L005\n- Lesson Title: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists\n- Module: M0 ‚Äî AI-Native Data Visualization Foundations\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M0-L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M0-L006",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M0-L006\n- Lesson Title: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals\n- Module: M0 ‚Äî AI-Native Data Visualization Foundations\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M0-L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M0-L007",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M0-L007\n- Lesson Title: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding\n- Module: M0 ‚Äî AI-Native Data Visualization Foundations\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M0-L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M0-L008",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M0-L008\n- Lesson Title: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì\n- Module: M0 ‚Äî AI-Native Data Visualization Foundations\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M0-L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M1-L001",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M1-L001\n- Lesson Title: Metric contracts (owner, formula, grain, filters, caveats)\n- Module: M1 ‚Äî Metric Contracts & Semantic Layer\n- Subtopic / Focus Area: General\n- Complexity Level: F (Foundation)\n- Estimated Duration: 45 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M1-L001: Metric contracts (owner, formula, grain, filters, caveats)` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M1-L002",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M1-L002\n- Lesson Title: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models)\n- Module: M1 ‚Äî Metric Contracts & Semantic Layer\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M1-L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models)` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M1-L003",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M1-L003\n- Lesson Title: Change management & versioning for metrics\n- Module: M1 ‚Äî Metric Contracts & Semantic Layer\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M1-L003: Change management & versioning for metrics` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M1-L004",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M1-L004\n- Lesson Title: Reconciliation across tools (SQL baselines vs BI outputs)\n- Module: M1 ‚Äî Metric Contracts & Semantic Layer\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M1-L004: Reconciliation across tools (SQL baselines vs BI outputs)` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M1-L005",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M1-L005\n- Lesson Title: Slowly changing definitions & re-baselining strategies\n- Module: M1 ‚Äî Metric Contracts & Semantic Layer\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M1-L005: Slowly changing definitions & re-baselining strategies` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M1-L006",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M1-L006\n- Lesson Title: Certification workflows & scorecards (gold/silver/experimental)\n- Module: M1 ‚Äî Metric Contracts & Semantic Layer\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M1-L006: Certification workflows & scorecards (gold/silver/experimental)` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M1-L007",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M1-L007\n- Lesson Title: Provenance storage & citation-first narratives\n- Module: M1 ‚Äî Metric Contracts & Semantic Layer\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M1-L007: Provenance storage & citation-first narratives` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M1-L008",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M1-L008\n- Lesson Title: Blast-radius analysis for metric changes\n- Module: M1 ‚Äî Metric Contracts & Semantic Layer\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M1-L008: Blast-radius analysis for metric changes` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M2-L001",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M2-L001\n- Lesson Title: Visual design tokens (color, spacing, typography) for BI\n- Module: M2 ‚Äî Design Systems & Accessibility\n- Subtopic / Focus Area: General\n- Complexity Level: F (Foundation)\n- Estimated Duration: 45 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M2-L001: Visual design tokens (color, spacing, typography) for BI` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M2-L002",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M2-L002\n- Lesson Title: Chart component library patterns & theming\n- Module: M2 ‚Äî Design Systems & Accessibility\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M2-L002: Chart component library patterns & theming` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M2-L003",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M2-L003\n- Lesson Title: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard)\n- Module: M2 ‚Äî Design Systems & Accessibility\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M2-L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard)` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M2-L004",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M2-L004\n- Lesson Title: Color-vision safe palettes & redundancy (shape/texture)\n- Module: M2 ‚Äî Design Systems & Accessibility\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M2-L004: Color-vision safe palettes & redundancy (shape/texture)` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M2-L005",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M2-L005\n- Lesson Title: Motion, animation, and vestibular safety\n- Module: M2 ‚Äî Design Systems & Accessibility\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M2-L005: Motion, animation, and vestibular safety` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M2-L006",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M2-L006\n- Lesson Title: Print/export safe styles & high-contrast modes\n- Module: M2 ‚Äî Design Systems & Accessibility\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M2-L006: Print/export safe styles & high-contrast modes` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M2-L007",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M2-L007\n- Lesson Title: Governance: design reviews, linting, and design debt\n- Module: M2 ‚Äî Design Systems & Accessibility\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M2-L007: Governance: design reviews, linting, and design debt` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M2-L008",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M2-L008\n- Lesson Title: Accessibility QA & assistive tech checks\n- Module: M2 ‚Äî Design Systems & Accessibility\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M2-L008: Accessibility QA & assistive tech checks` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M3-L001",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M3-L001\n- Lesson Title: Encodings: position, length, angle, area, color, texture\n- Module: M3 ‚Äî Perception & Chart Taxonomy\n- Subtopic / Focus Area: General\n- Complexity Level: F (Foundation)\n- Estimated Duration: 45 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M3-L001: Encodings: position, length, angle, area, color, texture` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M3-L002",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M3-L002\n- Lesson Title: Chart selection matrix (goal, data type, comparison count)\n- Module: M3 ‚Äî Perception & Chart Taxonomy\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M3-L002: Chart selection matrix (goal, data type, comparison count)` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M3-L003",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M3-L003\n- Lesson Title: Avoiding lies: axis baselines, area misuse, dual axes risks\n- Module: M3 ‚Äî Perception & Chart Taxonomy\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M3-L003: Avoiding lies: axis baselines, area misuse, dual axes risks` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M3-L004",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M3-L004\n- Lesson Title: Uncertainty & error bars, confidence bands, distributions\n- Module: M3 ‚Äî Perception & Chart Taxonomy\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M3-L004: Uncertainty & error bars, confidence bands, distributions` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M3-L005",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M3-L005\n- Lesson Title: Small multiples vs interaction for comparisons\n- Module: M3 ‚Äî Perception & Chart Taxonomy\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M3-L005: Small multiples vs interaction for comparisons` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M3-L006",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M3-L006\n- Lesson Title: Aggregation bias, Simpson‚Äôs paradox, survivorship bias\n- Module: M3 ‚Äî Perception & Chart Taxonomy\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M3-L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M3-L007",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M3-L007\n- Lesson Title: Sparklines, tilegrams, bullet charts, slope graphs\n- Module: M3 ‚Äî Perception & Chart Taxonomy\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M3-L007: Sparklines, tilegrams, bullet charts, slope graphs` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M3-L008",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M3-L008\n- Lesson Title: Annotative storytelling & callout patterns\n- Module: M3 ‚Äî Perception & Chart Taxonomy\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M3-L008: Annotative storytelling & callout patterns` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M4-L001",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M4-L001\n- Lesson Title: Audience mapping & decision context\n- Module: M4 ‚Äî Storytelling & Narrative\n- Subtopic / Focus Area: General\n- Complexity Level: F (Foundation)\n- Estimated Duration: 45 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M4-L001: Audience mapping & decision context` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M4-L002",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M4-L002\n- Lesson Title: Narrative scaffolds & executive summaries\n- Module: M4 ‚Äî Storytelling & Narrative\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M4-L002: Narrative scaffolds & executive summaries` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M4-L003",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M4-L003\n- Lesson Title: Contrastive explanations & counterfactuals\n- Module: M4 ‚Äî Storytelling & Narrative\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M4-L003: Contrastive explanations & counterfactuals` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M4-L004",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M4-L004\n- Lesson Title: Evidence packing: citations, footnotes, and source links\n- Module: M4 ‚Äî Storytelling & Narrative\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M4-L004: Evidence packing: citations, footnotes, and source links` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M4-L005",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M4-L005\n- Lesson Title: Risk & limitation sections to build trust\n- Module: M4 ‚Äî Storytelling & Narrative\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M4-L005: Risk & limitation sections to build trust` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M4-L006",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M4-L006\n- Lesson Title: Talking points generator for presenters\n- Module: M4 ‚Äî Storytelling & Narrative\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M4-L006: Talking points generator for presenters` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M4-L007",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M4-L007\n- Lesson Title: Bias/fairness guardrails in messaging\n- Module: M4 ‚Äî Storytelling & Narrative\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M4-L007: Bias/fairness guardrails in messaging` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M4-L008",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M4-L008\n- Lesson Title: Narrative evaluators & rubrics\n- Module: M4 ‚Äî Storytelling & Narrative\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M4-L008: Narrative evaluators & rubrics` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M5-L001",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M5-L001\n- Lesson Title: Choosing grains & rollups for charts\n- Module: M5 ‚Äî Data Prep for Visualization\n- Subtopic / Focus Area: General\n- Complexity Level: F (Foundation)\n- Estimated Duration: 45 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M5-L001: Choosing grains & rollups for charts` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M5-L002",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M5-L002\n- Lesson Title: Dimensional modeling for BI (facts/dimensions, conformed dims)\n- Module: M5 ‚Äî Data Prep for Visualization\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M5-L002: Dimensional modeling for BI (facts/dimensions, conformed dims)` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M5-L003",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M5-L003\n- Lesson Title: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita)\n- Module: M5 ‚Äî Data Prep for Visualization\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M5-L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita)` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M5-L004",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M5-L004\n- Lesson Title: Window functions & percentiles for distributions\n- Module: M5 ‚Äî Data Prep for Visualization\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M5-L004: Window functions & percentiles for distributions` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M5-L005",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M5-L005\n- Lesson Title: Time series shaping (calendar tables, week/year variants)\n- Module: M5 ‚Äî Data Prep for Visualization\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M5-L005: Time series shaping (calendar tables, week/year variants)` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M5-L006",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M5-L006\n- Lesson Title: Pre-aggregation/extract strategies vs live query\n- Module: M5 ‚Äî Data Prep for Visualization\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M5-L006: Pre-aggregation/extract strategies vs live query` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M5-L007",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M5-L007\n- Lesson Title: Sampling & sketching (hyperloglog, t‚Äëdigest)\n- Module: M5 ‚Äî Data Prep for Visualization\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M5-L007: Sampling & sketching (hyperloglog, t‚Äëdigest)` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M5-L008",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M5-L008\n- Lesson Title: Quality checks & guardrails before render\n- Module: M5 ‚Äî Data Prep for Visualization\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M5-L008: Quality checks & guardrails before render` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M6-L001",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M6-L001\n- Lesson Title: Row-level & column-level security patterns in BI tools\n- Module: M6 ‚Äî Security & Privacy\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M6-L001: Row-level & column-level security patterns in BI tools` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M6-L002",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M6-L002\n- Lesson Title: Entitlement-aware queries & token scoping\n- Module: M6 ‚Äî Security & Privacy\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M6-L002: Entitlement-aware queries & token scoping` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M6-L003",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M6-L003\n- Lesson Title: Masking & redaction for screens/exports\n- Module: M6 ‚Äî Security & Privacy\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M6-L003: Masking & redaction for screens/exports` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M6-L004",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M6-L004\n- Lesson Title: Consent/purpose limitation in embedded contexts\n- Module: M6 ‚Äî Security & Privacy\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M6-L004: Consent/purpose limitation in embedded contexts` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M6-L005",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M6-L005\n- Lesson Title: Audit trails for views, exports & shares\n- Module: M6 ‚Äî Security & Privacy\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M6-L005: Audit trails for views, exports & shares` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M6-L006",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M6-L006\n- Lesson Title: Secrets & signed URLs for tiles/embeds\n- Module: M6 ‚Äî Security & Privacy\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M6-L006: Secrets & signed URLs for tiles/embeds` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M6-L007",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M6-L007\n- Lesson Title: Pseudonymization & small‚Äën suppression\n- Module: M6 ‚Äî Security & Privacy\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M6-L007: Pseudonymization & small‚Äën suppression` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M6-L008",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M6-L008\n- Lesson Title: Privacy incident playbooks for BI\n- Module: M6 ‚Äî Security & Privacy\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M6-L008: Privacy incident playbooks for BI` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M7-L001",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M7-L001\n- Lesson Title: Layout grids, density, and scan patterns (F/Z)\n- Module: M7 ‚Äî Dashboard Engineering\n- Subtopic / Focus Area: General\n- Complexity Level: F (Foundation)\n- Estimated Duration: 45 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M7-L001: Layout grids, density, and scan patterns (F/Z)` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M7-L002",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M7-L002\n- Lesson Title: Filter/state management (global vs scoped)\n- Module: M7 ‚Äî Dashboard Engineering\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M7-L002: Filter/state management (global vs scoped)` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M7-L003",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M7-L003\n- Lesson Title: Drill-down/drill-through & linked highlighting\n- Module: M7 ‚Äî Dashboard Engineering\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M7-L003: Drill-down/drill-through & linked highlighting` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M7-L004",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M7-L004\n- Lesson Title: Parameterization & what-if controls\n- Module: M7 ‚Äî Dashboard Engineering\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M7-L004: Parameterization & what-if controls` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M7-L005",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M7-L005\n- Lesson Title: Mobile/responsive/adaptive breakpoints\n- Module: M7 ‚Äî Dashboard Engineering\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M7-L005: Mobile/responsive/adaptive breakpoints` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M7-L006",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M7-L006\n- Lesson Title: Offline/low‚Äëbandwidth fallbacks & progressive disclosure\n- Module: M7 ‚Äî Dashboard Engineering\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M7-L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M7-L007",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M7-L007\n- Lesson Title: Internationalization: locale, number, date, currency, bidi\n- Module: M7 ‚Äî Dashboard Engineering\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M7-L007: Internationalization: locale, number, date, currency, bidi` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M7-L008",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M7-L008\n- Lesson Title: Component contracts & reusability\n- Module: M7 ‚Äî Dashboard Engineering\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M7-L008: Component contracts & reusability` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M8-L001",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M8-L001\n- Lesson Title: Perf budgets for pages, charts, queries\n- Module: M8 ‚Äî Performance & Scale\n- Subtopic / Focus Area: General\n- Complexity Level: F (Foundation)\n- Estimated Duration: 45 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M8-L001: Perf budgets for pages, charts, queries` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M8-L002",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M8-L002\n- Lesson Title: Cache hierarchy (client/edge/app/DB/BI extracts)\n- Module: M8 ‚Äî Performance & Scale\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M8-L002: Cache hierarchy (client/edge/app/DB/BI extracts)` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M8-L003",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M8-L003\n- Lesson Title: Aggregation tables/materialized views\n- Module: M8 ‚Äî Performance & Scale\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M8-L003: Aggregation tables/materialized views` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M8-L004",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M8-L004\n- Lesson Title: Query tuning (pruning, partition filters, indexes)\n- Module: M8 ‚Äî Performance & Scale\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M8-L004: Query tuning (pruning, partition filters, indexes)` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M8-L005",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M8-L005\n- Lesson Title: Incremental refresh & cache invalidation\n- Module: M8 ‚Äî Performance & Scale\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M8-L005: Incremental refresh & cache invalidation` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M8-L006",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M8-L006\n- Lesson Title: Tile pre-rendering & snapshot scheduling\n- Module: M8 ‚Äî Performance & Scale\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M8-L006: Tile pre-rendering & snapshot scheduling` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M8-L007",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M8-L007\n- Lesson Title: Tail latency analysis & p95/p99 dashboards\n- Module: M8 ‚Äî Performance & Scale\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M8-L007: Tail latency analysis & p95/p99 dashboards` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M8-L008",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M8-L008\n- Lesson Title: Cost/perf experiments & SLOs\n- Module: M8 ‚Äî Performance & Scale\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M8-L008: Cost/perf experiments & SLOs` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M9-L001",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M9-L001\n- Lesson Title: Platform selection matrix & constraints\n- Module: M9 ‚Äî BI Platforms\n- Subtopic / Focus Area: General\n- Complexity Level: F (Foundation)\n- Estimated Duration: 45 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M9-L001: Platform selection matrix & constraints` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M9-L002",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M9-L002\n- Lesson Title: Tableau: extracts, LOD, actions, server governance\n- Module: M9 ‚Äî BI Platforms\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M9-L002: Tableau: extracts, LOD, actions, server governance` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M9-L003",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M9-L003\n- Lesson Title: Power BI: models, DAX, gateways, deployment pipelines\n- Module: M9 ‚Äî BI Platforms\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M9-L003: Power BI: models, DAX, gateways, deployment pipelines` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M9-L004",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M9-L004\n- Lesson Title: Looker: LookML, PDTs, explores, caching\n- Module: M9 ‚Äî BI Platforms\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M9-L004: Looker: LookML, PDTs, explores, caching` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M9-L005",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M9-L005\n- Lesson Title: ThoughtSpot: pinboards, answer search, worksheets\n- Module: M9 ‚Äî BI Platforms\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M9-L005: ThoughtSpot: pinboards, answer search, worksheets` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M9-L006",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M9-L006\n- Lesson Title: Admin/tenant governance & capacity planning\n- Module: M9 ‚Äî BI Platforms\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M9-L006: Admin/tenant governance & capacity planning` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M9-L007",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M9-L007\n- Lesson Title: Security/embedding per platform\n- Module: M9 ‚Äî BI Platforms\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M9-L007: Security/embedding per platform` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M9-L008",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M9-L008\n- Lesson Title: Interop & coexistence strategies\n- Module: M9 ‚Äî BI Platforms\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M9-L008: Interop & coexistence strategies` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M10-L001",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M10-L001\n- Lesson Title: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL)\n- Module: M10 ‚Äî Web Visualization Stack\n- Subtopic / Focus Area: General\n- Complexity Level: F (Foundation)\n- Estimated Duration: 45 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M10-L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL)` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M10-L002",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M10-L002\n- Lesson Title: Dataflow & state management for custom viz\n- Module: M10 ‚Äî Web Visualization Stack\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M10-L002: Dataflow & state management for custom viz` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M10-L003",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M10-L003\n- Lesson Title: Scale systems, axes, and interaction frameworks\n- Module: M10 ‚Äî Web Visualization Stack\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M10-L003: Scale systems, axes, and interaction frameworks` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M10-L004",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M10-L004\n- Lesson Title: High‚Äëdensity rendering with Canvas/WebGL\n- Module: M10 ‚Äî Web Visualization Stack\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M10-L004: High‚Äëdensity rendering with Canvas/WebGL` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M10-L005",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M10-L005\n- Lesson Title: Performance profiling & flamegraphs\n- Module: M10 ‚Äî Web Visualization Stack\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M10-L005: Performance profiling & flamegraphs` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M10-L006",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M10-L006\n- Lesson Title: Accessibility in custom components\n- Module: M10 ‚Äî Web Visualization Stack\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M10-L006: Accessibility in custom components` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M10-L007",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M10-L007\n- Lesson Title: Testing: unit, screenshot diffs, storybook\n- Module: M10 ‚Äî Web Visualization Stack\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M10-L007: Testing: unit, screenshot diffs, storybook` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M10-L008",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M10-L008\n- Lesson Title: Packaging, theming, and versioning\n- Module: M10 ‚Äî Web Visualization Stack\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M10-L008: Packaging, theming, and versioning` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M11-L001",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M11-L001\n- Lesson Title: Projections & distortions; when to use which\n- Module: M11 ‚Äî Geospatial & Mapping\n- Subtopic / Focus Area: General\n- Complexity Level: F (Foundation)\n- Estimated Duration: 45 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M11-L001: Projections & distortions; when to use which` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M11-L002",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M11-L002\n- Lesson Title: Vector vs raster tiles; providers & caching\n- Module: M11 ‚Äî Geospatial & Mapping\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M11-L002: Vector vs raster tiles; providers & caching` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M11-L003",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M11-L003\n- Lesson Title: Choropleths, hexbin, dot density, flow maps\n- Module: M11 ‚Äî Geospatial & Mapping\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M11-L003: Choropleths, hexbin, dot density, flow maps` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M11-L004",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M11-L004\n- Lesson Title: Geocoding, reverse geocoding & privacy\n- Module: M11 ‚Äî Geospatial & Mapping\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M11-L004: Geocoding, reverse geocoding & privacy` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M11-L005",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M11-L005\n- Lesson Title: Clustering & server‚Äëside binning at scale\n- Module: M11 ‚Äî Geospatial & Mapping\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M11-L005: Clustering & server‚Äëside binning at scale` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M11-L006",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M11-L006\n- Lesson Title: Map interactions & snapping tolerances\n- Module: M11 ‚Äî Geospatial & Mapping\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M11-L006: Map interactions & snapping tolerances` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M11-L007",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M11-L007\n- Lesson Title: Mapbox/Leaflet/Kepler.gl patterns\n- Module: M11 ‚Äî Geospatial & Mapping\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M11-L007: Mapbox/Leaflet/Kepler.gl patterns` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M11-L008",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M11-L008\n- Lesson Title: Performance & attribution requirements\n- Module: M11 ‚Äî Geospatial & Mapping\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M11-L008: Performance & attribution requirements` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M12-L001",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M12-L001\n- Lesson Title: WebSockets/SSE & backpressure in UIs\n- Module: M12 ‚Äî Real-Time & Streaming Visualization\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M12-L001: WebSockets/SSE & backpressure in UIs` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M12-L002",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M12-L002\n- Lesson Title: Incremental rendering & windowing strategies\n- Module: M12 ‚Äî Real-Time & Streaming Visualization\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M12-L002: Incremental rendering & windowing strategies` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M12-L003",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M12-L003\n- Lesson Title: Event ordering & out‚Äëof‚Äëorder handling visually\n- Module: M12 ‚Äî Real-Time & Streaming Visualization\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M12-L003: Event ordering & out‚Äëof‚Äëorder handling visually` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M12-L004",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M12-L004\n- Lesson Title: Alert thresholds & sparklines for tails\n- Module: M12 ‚Äî Real-Time & Streaming Visualization\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M12-L004: Alert thresholds & sparklines for tails` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M12-L005",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M12-L005\n- Lesson Title: Streaming aggregates & watermarking\n- Module: M12 ‚Äî Real-Time & Streaming Visualization\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M12-L005: Streaming aggregates & watermarking` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M12-L006",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M12-L006\n- Lesson Title: Rate‚Äëlimit handling & fallbacks\n- Module: M12 ‚Äî Real-Time & Streaming Visualization\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M12-L006: Rate‚Äëlimit handling & fallbacks` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M12-L007",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M12-L007\n- Lesson Title: Incident dashboards & noise controls\n- Module: M12 ‚Äî Real-Time & Streaming Visualization\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M12-L007: Incident dashboards & noise controls` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M12-L008",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M12-L008\n- Lesson Title: Evidence: link charts to log/trace exemplars\n- Module: M12 ‚Äî Real-Time & Streaming Visualization\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M12-L008: Evidence: link charts to log/trace exemplars` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M13-L001",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M13-L001\n- Lesson Title: Crossfilter principles & brushing/linking\n- Module: M13 ‚Äî Interactive Analytics\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M13-L001: Crossfilter principles & brushing/linking` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M13-L002",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M13-L002\n- Lesson Title: Level‚Äëof‚Äëdetail controls and pitfalls\n- Module: M13 ‚Äî Interactive Analytics\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M13-L002: Level‚Äëof‚Äëdetail controls and pitfalls` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M13-L003",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M13-L003\n- Lesson Title: Ad‚Äëhoc builders with guardrails\n- Module: M13 ‚Äî Interactive Analytics\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M13-L003: Ad‚Äëhoc builders with guardrails` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M13-L004",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M13-L004\n- Lesson Title: Session state & undo/redo history\n- Module: M13 ‚Äî Interactive Analytics\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M13-L004: Session state & undo/redo history` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M13-L005",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M13-L005\n- Lesson Title: Query shaping & safe limits for ad‚Äëhoc\n- Module: M13 ‚Äî Interactive Analytics\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M13-L005: Query shaping & safe limits for ad‚Äëhoc` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M13-L006",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M13-L006\n- Lesson Title: Explainers & ‚Äúwhy this point‚Äù probes\n- Module: M13 ‚Äî Interactive Analytics\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M13-L006: Explainers & ‚Äúwhy this point‚Äù probes` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M13-L007",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M13-L007\n- Lesson Title: Save/share reproducible states\n- Module: M13 ‚Äî Interactive Analytics\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M13-L007: Save/share reproducible states` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M13-L008",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M13-L008\n- Lesson Title: Evidence packs for analyst reviews\n- Module: M13 ‚Äî Interactive Analytics\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M13-L008: Evidence packs for analyst reviews` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M14-L001",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M14-L001\n- Lesson Title: Safe prompt templates for chart narration\n- Module: M14 ‚Äî Explainable Insights & LLM Copilots\n- Subtopic / Focus Area: General\n- Complexity Level: F (Foundation)\n- Estimated Duration: 45 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M14-L001: Safe prompt templates for chart narration` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M14-L002",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M14-L002\n- Lesson Title: Grounding on metric canonicals & query SQL with citations\n- Module: M14 ‚Äî Explainable Insights & LLM Copilots\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M14-L002: Grounding on metric canonicals & query SQL with citations` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M14-L003",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M14-L003\n- Lesson Title: Insight ranking & deduplication (avoid trivia)\n- Module: M14 ‚Äî Explainable Insights & LLM Copilots\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M14-L003: Insight ranking & deduplication (avoid trivia)` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M14-L004",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M14-L004\n- Lesson Title: Counterfactual & ‚Äúwhat changed‚Äù narratives\n- Module: M14 ‚Äî Explainable Insights & LLM Copilots\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M14-L004: Counterfactual & ‚Äúwhat changed‚Äù narratives` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M14-L005",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M14-L005\n- Lesson Title: Uncertainty-aware statements & caveats\n- Module: M14 ‚Äî Explainable Insights & LLM Copilots\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M14-L005: Uncertainty-aware statements & caveats` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M14-L006",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M14-L006\n- Lesson Title: Cost/latency budgets & fallbacks\n- Module: M14 ‚Äî Explainable Insights & LLM Copilots\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M14-L006: Cost/latency budgets & fallbacks` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M14-L007",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M14-L007\n- Lesson Title: Evaluators: faithfulness, usefulness, actionability\n- Module: M14 ‚Äî Explainable Insights & LLM Copilots\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M14-L007: Evaluators: faithfulness, usefulness, actionability` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M14-L008",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M14-L008\n- Lesson Title: Versioning/drift monitoring for explainers\n- Module: M14 ‚Äî Explainable Insights & LLM Copilots\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M14-L008: Versioning/drift monitoring for explainers` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M15-L001",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M15-L001\n- Lesson Title: Golden views & snapshot diffs (image hash, SSIM)\n- Module: M15 ‚Äî Evaluation & QA\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M15-L001: Golden views & snapshot diffs (image hash, SSIM)` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M15-L002",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M15-L002\n- Lesson Title: Data‚Äëdriven tests: totals, filters, highlights\n- Module: M15 ‚Äî Evaluation & QA\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M15-L002: Data‚Äëdriven tests: totals, filters, highlights` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M15-L003",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M15-L003\n- Lesson Title: Accessibility audits automated in CI\n- Module: M15 ‚Äî Evaluation & QA\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M15-L003: Accessibility audits automated in CI` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M15-L004",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M15-L004\n- Lesson Title: Perf gates for tiles & queries\n- Module: M15 ‚Äî Evaluation & QA\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M15-L004: Perf gates for tiles & queries` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M15-L005",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M15-L005\n- Lesson Title: Change detection & alerting on dashboards\n- Module: M15 ‚Äî Evaluation & QA\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M15-L005: Change detection & alerting on dashboards` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M15-L006",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M15-L006\n- Lesson Title: Reviewer checklists & sign‚Äëoffs\n- Module: M15 ‚Äî Evaluation & QA\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M15-L006: Reviewer checklists & sign‚Äëoffs` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M15-L007",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M15-L007\n- Lesson Title: Post‚Äërelease quality monitors\n- Module: M15 ‚Äî Evaluation & QA\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M15-L007: Post‚Äërelease quality monitors` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M15-L008",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M15-L008\n- Lesson Title: Incident‚Üíimprovement loops\n- Module: M15 ‚Äî Evaluation & QA\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M15-L008: Incident‚Üíimprovement loops` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M16-L001",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M16-L001\n- Lesson Title: Event taxonomy (view, hover, filter, export)\n- Module: M16 ‚Äî Observability & Product Analytics\n- Subtopic / Focus Area: General\n- Complexity Level: F (Foundation)\n- Estimated Duration: 45 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M16-L001: Event taxonomy (view, hover, filter, export)` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M16-L002",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M16-L002\n- Lesson Title: Funnels & drop‚Äëoff analysis for dashboards\n- Module: M16 ‚Äî Observability & Product Analytics\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M16-L002: Funnels & drop‚Äëoff analysis for dashboards` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M16-L003",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M16-L003\n- Lesson Title: NPS/CSAT overlays & feedback capture\n- Module: M16 ‚Äî Observability & Product Analytics\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M16-L003: NPS/CSAT overlays & feedback capture` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M16-L004",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M16-L004\n- Lesson Title: Correlate usage to decisions & business KPIs\n- Module: M16 ‚Äî Observability & Product Analytics\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M16-L004: Correlate usage to decisions & business KPIs` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M16-L005",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M16-L005\n- Lesson Title: Content lifecycle: stale, merge, archive\n- Module: M16 ‚Äî Observability & Product Analytics\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M16-L005: Content lifecycle: stale, merge, archive` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M16-L006",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M16-L006\n- Lesson Title: Ownership & on‚Äëcall for critical dashboards\n- Module: M16 ‚Äî Observability & Product Analytics\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M16-L006: Ownership & on‚Äëcall for critical dashboards` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M16-L007",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M16-L007\n- Lesson Title: Executive adoption scorecards\n- Module: M16 ‚Äî Observability & Product Analytics\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M16-L007: Executive adoption scorecards` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M16-L008",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M16-L008\n- Lesson Title: Quarterly content reviews & rationalization\n- Module: M16 ‚Äî Observability & Product Analytics\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M16-L008: Quarterly content reviews & rationalization` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M17-L001",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M17-L001\n- Lesson Title: Hypothesis design & success metrics\n- Module: M17 ‚Äî Experimentation & A/B\n- Subtopic / Focus Area: General\n- Complexity Level: F (Foundation)\n- Estimated Duration: 45 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M17-L001: Hypothesis design & success metrics` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M17-L002",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M17-L002\n- Lesson Title: Sample size/power & SRM checks\n- Module: M17 ‚Äî Experimentation & A/B\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M17-L002: Sample size/power & SRM checks` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M17-L003",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M17-L003\n- Lesson Title: Variant toggles & rollout plans\n- Module: M17 ‚Äî Experimentation & A/B\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M17-L003: Variant toggles & rollout plans` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M17-L004",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M17-L004\n- Lesson Title: Attribution pitfalls & learning agendas\n- Module: M17 ‚Äî Experimentation & A/B\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M17-L004: Attribution pitfalls & learning agendas` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M17-L005",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M17-L005\n- Lesson Title: Ethics & eligibility constraints\n- Module: M17 ‚Äî Experimentation & A/B\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M17-L005: Ethics & eligibility constraints` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M17-L006",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M17-L006\n- Lesson Title: Analysis & practical significance\n- Module: M17 ‚Äî Experimentation & A/B\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M17-L006: Analysis & practical significance` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M17-L007",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M17-L007\n- Lesson Title: Decision memos & rollout criteria\n- Module: M17 ‚Äî Experimentation & A/B\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M17-L007: Decision memos & rollout criteria` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M17-L008",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M17-L008\n- Lesson Title: Archive of experiments & learnings\n- Module: M17 ‚Äî Experimentation & A/B\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M17-L008: Archive of experiments & learnings` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M18-L001",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M18-L001\n- Lesson Title: Export contracts (fonts, layout, watermarking)\n- Module: M18 ‚Äî Export, Print & Board Packs\n- Subtopic / Focus Area: General\n- Complexity Level: F (Foundation)\n- Estimated Duration: 45 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M18-L001: Export contracts (fonts, layout, watermarking)` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M18-L002",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M18-L002\n- Lesson Title: PDF/slide automation; pagination rules\n- Module: M18 ‚Äî Export, Print & Board Packs\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M18-L002: PDF/slide automation; pagination rules` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M18-L003",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M18-L003\n- Lesson Title: Board/MBR/QBR pack templates & storylines\n- Module: M18 ‚Äî Export, Print & Board Packs\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M18-L003: Board/MBR/QBR pack templates & storylines` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M18-L004",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M18-L004\n- Lesson Title: Evidence appendix & footnotes generator\n- Module: M18 ‚Äî Export, Print & Board Packs\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M18-L004: Evidence appendix & footnotes generator` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M18-L005",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M18-L005\n- Lesson Title: Change logs & release notes for leadership\n- Module: M18 ‚Äî Export, Print & Board Packs\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M18-L005: Change logs & release notes for leadership` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M18-L006",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M18-L006\n- Lesson Title: Redaction & small‚Äën suppression in exports\n- Module: M18 ‚Äî Export, Print & Board Packs\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M18-L006: Redaction & small‚Äën suppression in exports` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M18-L007",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M18-L007\n- Lesson Title: Translation/I18n for packs\n- Module: M18 ‚Äî Export, Print & Board Packs\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M18-L007: Translation/I18n for packs` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M18-L008",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M18-L008\n- Lesson Title: Distribution workflows & access logs\n- Module: M18 ‚Äî Export, Print & Board Packs\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M18-L008: Distribution workflows & access logs` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M19-L001",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M19-L001\n- Lesson Title: Locale‚Äëaware numbers, dates, calendars\n- Module: M19 ‚Äî Internationalization & Localization\n- Subtopic / Focus Area: General\n- Complexity Level: F (Foundation)\n- Estimated Duration: 45 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M19-L001: Locale‚Äëaware numbers, dates, calendars` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M19-L002",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M19-L002\n- Lesson Title: Currency conversions & rounding policies\n- Module: M19 ‚Äî Internationalization & Localization\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M19-L002: Currency conversions & rounding policies` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M19-L003",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M19-L003\n- Lesson Title: RTL/bidi layout adjustments\n- Module: M19 ‚Äî Internationalization & Localization\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M19-L003: RTL/bidi layout adjustments` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M19-L004",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M19-L004\n- Lesson Title: Multilingual labels & search\n- Module: M19 ‚Äî Internationalization & Localization\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M19-L004: Multilingual labels & search` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M19-L005",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M19-L005\n- Lesson Title: Unit systems (metric/imperial)\n- Module: M19 ‚Äî Internationalization & Localization\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M19-L005: Unit systems (metric/imperial)` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M19-L006",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M19-L006\n- Lesson Title: Cultural chart conventions & risks\n- Module: M19 ‚Äî Internationalization & Localization\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M19-L006: Cultural chart conventions & risks` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M19-L007",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M19-L007\n- Lesson Title: Timezone normalization strategies\n- Module: M19 ‚Äî Internationalization & Localization\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M19-L007: Timezone normalization strategies` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M19-L008",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M19-L008\n- Lesson Title: QA checklists for localization\n- Module: M19 ‚Äî Internationalization & Localization\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M19-L008: QA checklists for localization` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M20-L001",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M20-L001\n- Lesson Title: Embedding patterns (iFrame, JS SDK, REST tiles)\n- Module: M20 ‚Äî Embedding & App Integration\n- Subtopic / Focus Area: General\n- Complexity Level: F (Foundation)\n- Estimated Duration: 45 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M20-L001: Embedding patterns (iFrame, JS SDK, REST tiles)` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M20-L002",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M20-L002\n- Lesson Title: SSO & token exchange (SAML/OIDC/JWT)\n- Module: M20 ‚Äî Embedding & App Integration\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M20-L002: SSO & token exchange (SAML/OIDC/JWT)` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M20-L003",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M20-L003\n- Lesson Title: Row‚Äëlevel security propagation to embeds\n- Module: M20 ‚Äî Embedding & App Integration\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M20-L003: Row‚Äëlevel security propagation to embeds` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M20-L004",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M20-L004\n- Lesson Title: Event APIs for host‚Äëapp integration\n- Module: M20 ‚Äî Embedding & App Integration\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M20-L004: Event APIs for host‚Äëapp integration` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M20-L005",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M20-L005\n- Lesson Title: Multi‚Äëtenant & white‚Äëlabel strategies\n- Module: M20 ‚Äî Embedding & App Integration\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M20-L005: Multi‚Äëtenant & white‚Äëlabel strategies` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M20-L006",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M20-L006\n- Lesson Title: Offline snapshots for field teams\n- Module: M20 ‚Äî Embedding & App Integration\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M20-L006: Offline snapshots for field teams` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M20-L007",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M20-L007\n- Lesson Title: Mobile SDKs & secure storage\n- Module: M20 ‚Äî Embedding & App Integration\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M20-L007: Mobile SDKs & secure storage` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M20-L008",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M20-L008\n- Lesson Title: Contract tests for embedded integrations\n- Module: M20 ‚Äî Embedding & App Integration\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M20-L008: Contract tests for embedded integrations` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M21-L001",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M21-L001\n- Lesson Title: Cost primitives (compute, storage, egress, API/tokens)\n- Module: M21 ‚Äî FinOps & Cost Governance\n- Subtopic / Focus Area: General\n- Complexity Level: F (Foundation)\n- Estimated Duration: 45 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M21-L001: Cost primitives (compute, storage, egress, API/tokens)` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M21-L002",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M21-L002\n- Lesson Title: Budget alerts by dash/dataset/team\n- Module: M21 ‚Äî FinOps & Cost Governance\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M21-L002: Budget alerts by dash/dataset/team` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M21-L003",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M21-L003\n- Lesson Title: Pruning, extracts & cache hit strategies\n- Module: M21 ‚Äî FinOps & Cost Governance\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M21-L003: Pruning, extracts & cache hit strategies` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M21-L004",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M21-L004\n- Lesson Title: Expensive‚Äëquery detector & refactors\n- Module: M21 ‚Äî FinOps & Cost Governance\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M21-L004: Expensive‚Äëquery detector & refactors` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M21-L005",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M21-L005\n- Lesson Title: Multi‚Äëregion egress & CDN cost controls\n- Module: M21 ‚Äî FinOps & Cost Governance\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M21-L005: Multi‚Äëregion egress & CDN cost controls` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M21-L006",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M21-L006\n- Lesson Title: Chargeback/showback & utilization KPIs\n- Module: M21 ‚Äî FinOps & Cost Governance\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M21-L006: Chargeback/showback & utilization KPIs` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M21-L007",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M21-L007\n- Lesson Title: Vendor negotiations & renewal readiness\n- Module: M21 ‚Äî FinOps & Cost Governance\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M21-L007: Vendor negotiations & renewal readiness` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M21-L008",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M21-L008\n- Lesson Title: Quarterly value reports & savings narratives\n- Module: M21 ‚Äî FinOps & Cost Governance\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M21-L008: Quarterly value reports & savings narratives` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M22-L001",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M22-L001\n- Lesson Title: Asset catalog & ownership registry\n- Module: M22 ‚Äî Governance & Lifecycle\n- Subtopic / Focus Area: General\n- Complexity Level: F (Foundation)\n- Estimated Duration: 45 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M22-L001: Asset catalog & ownership registry` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M22-L002",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M22-L002\n- Lesson Title: Content certification & deprecation windows\n- Module: M22 ‚Äî Governance & Lifecycle\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M22-L002: Content certification & deprecation windows` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M22-L003",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M22-L003\n- Lesson Title: ADRs & decision logs for dashboards\n- Module: M22 ‚Äî Governance & Lifecycle\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M22-L003: ADRs & decision logs for dashboards` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M22-L004",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M22-L004\n- Lesson Title: Request intake, triage & SLAs\n- Module: M22 ‚Äî Governance & Lifecycle\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M22-L004: Request intake, triage & SLAs` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M22-L005",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M22-L005\n- Lesson Title: Security reviews & privacy overlays\n- Module: M22 ‚Äî Governance & Lifecycle\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M22-L005: Security reviews & privacy overlays` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M22-L006",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M22-L006\n- Lesson Title: Release trains & freeze windows\n- Module: M22 ‚Äî Governance & Lifecycle\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M22-L006: Release trains & freeze windows` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M22-L007",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M22-L007\n- Lesson Title: Shadow‚ÄëIT detection & remediation\n- Module: M22 ‚Äî Governance & Lifecycle\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M22-L007: Shadow‚ÄëIT detection & remediation` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M22-L008",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M22-L008\n- Lesson Title: Annual portfolio review & roadmap\n- Module: M22 ‚Äî Governance & Lifecycle\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M22-L008: Annual portfolio review & roadmap` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M23-L001",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M23-L001\n- Lesson Title: Sensitive attributes & fairness caveats\n- Module: M23 ‚Äî Ethics, Bias & Misleading Visualization\n- Subtopic / Focus Area: General\n- Complexity Level: F (Foundation)\n- Estimated Duration: 45 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M23-L001: Sensitive attributes & fairness caveats` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M23-L002",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M23-L002\n- Lesson Title: Deceptive patterns & anti‚Äëpatterns catalog\n- Module: M23 ‚Äî Ethics, Bias & Misleading Visualization\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M23-L002: Deceptive patterns & anti‚Äëpatterns catalog` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M23-L003",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M23-L003\n- Lesson Title: Aggregation bias & subgroup harm analyses\n- Module: M23 ‚Äî Ethics, Bias & Misleading Visualization\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M23-L003: Aggregation bias & subgroup harm analyses` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M23-L004",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M23-L004\n- Lesson Title: Uncertainty visualization & disclosure norms\n- Module: M23 ‚Äî Ethics, Bias & Misleading Visualization\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M23-L004: Uncertainty visualization & disclosure norms` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M23-L005",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M23-L005\n- Lesson Title: Privacy‚Äëpreserving visualizations (noise, bins)\n- Module: M23 ‚Äî Ethics, Bias & Misleading Visualization\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M23-L005: Privacy‚Äëpreserving visualizations (noise, bins)` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M23-L006",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M23-L006\n- Lesson Title: Policy/ethics board & escalation paths\n- Module: M23 ‚Äî Ethics, Bias & Misleading Visualization\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M23-L006: Policy/ethics board & escalation paths` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M23-L007",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M23-L007\n- Lesson Title: Reviewer attestations & sign‚Äëoffs\n- Module: M23 ‚Äî Ethics, Bias & Misleading Visualization\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M23-L007: Reviewer attestations & sign‚Äëoffs` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M23-L008",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M23-L008\n- Lesson Title: Public/exec disclosures with guardrails\n- Module: M23 ‚Äî Ethics, Bias & Misleading Visualization\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M23-L008: Public/exec disclosures with guardrails` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M24-L001",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M24-L001\n- Lesson Title: Charter: audience, decisions, KPIs, risks, success criteria\n- Module: M24 ‚Äî Capstone\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M24-L001: Charter: audience, decisions, KPIs, risks, success criteria` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M24-L002",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M24-L002\n- Lesson Title: Metric contracts & semantic layer with certification\n- Module: M24 ‚Äî Capstone\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M24-L002: Metric contracts & semantic layer with certification` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M24-L003",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M24-L003\n- Lesson Title: Data prep & performance plan (extracts, caches, budgets)\n- Module: M24 ‚Äî Capstone\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M24-L003: Data prep & performance plan (extracts, caches, budgets)` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M24-L004",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M24-L004\n- Lesson Title: Accessibility‚Äëcompliant design system & components\n- Module: M24 ‚Äî Capstone\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M24-L004: Accessibility‚Äëcompliant design system & components` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M24-L005",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M24-L005\n- Lesson Title: LLM explainers grounded on SQL/metrics with citations\n- Module: M24 ‚Äî Capstone\n- Subtopic / Focus Area: General\n- Complexity Level: A (Advanced)\n- Estimated Duration: 75 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M24-L005: LLM explainers grounded on SQL/metrics with citations` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M24-L006",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M24-L006\n- Lesson Title: QA harness (visual diffs, metric checks, a11y)\n- Module: M24 ‚Äî Capstone\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M24-L006: QA harness (visual diffs, metric checks, a11y)` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M24-L007",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M24-L007\n- Lesson Title: Observability & adoption dashboards; A/B on story variants\n- Module: M24 ‚Äî Capstone\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M24-L007: Observability & adoption dashboards; A/B on story variants` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    },
    {
      "custom_id": "lesson_M24-L008",
      "params": {
        "model": "claude-haiku-4-5-20251001",
        "max_tokens": 16384,
        "messages": [
          {
            "role": "user",
            "content": "You are generating a polished, publication-ready lesson for a Technical Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n### CONTENT STRUCTURE REFERENCE (Curriculum Map - first 8000 chars for context):\nComplexity Legend\n\n[F] Foundational ‚Äî Basic concepts, prerequisites\n[I] Intermediate ‚Äî Practical application, integration of concepts\n[A] Advanced ‚Äî Complex problem-solving, experience-dependent\n[E] Expert ‚Äî Specialized knowledge, architectural decisions\n\n\n# M0: AI-Native Data Visualization Foundations ‚Äî Lesson Map - Core Concepts & Roles\n\n## L001: What AI Changes in Data Visualization (from charts ‚Üí decision narratives with evidence) [F]\n## L002: Roles in the AI Stack: Viz Engineer, Analytics Engineer, Product Analyst, Designer, DX/UX, Sec, PM [F]\n## L003: Prompt Patterns for Viz (context, metric contracts, constraints, audiences, risks, next steps) [F]\n## L004: RAG for Viz: pulling metric definitions/policies/MBR decks with citations [I]\n## L005: Copilot Workbench: chart suggestions, explainer text, insight ranking, QA checklists [I]\n## L006: Evaluators & Rubrics: clarity, accuracy, fairness, accessibility, and trust signals [I]\n## L007: Guardrails: PII minimization, redaction-by-default, entitlement-aware data binding [A]\n## L008: Outcome Metrics: time-to-insight‚Üì, adoption‚Üë, decision confidence‚Üë, query cost/report‚Üì [I]\n\n\n# M1: Metric Contracts & Semantic Layer ‚Äî Lesson Map - Definitions, Lineage & Trust\n\n## L001: Metric contracts (owner, formula, grain, filters, caveats) [F]\n## L002: Semantic layer patterns (dbt metrics, LookML, Tableau LOD, Power BI semantic models) [I]\n## L003: Change management & versioning for metrics [I]\n## L004: Reconciliation across tools (SQL baselines vs BI outputs) [I]\n## L005: Slowly changing definitions & re-baselining strategies [A]\n## L006: Certification workflows & scorecards (gold/silver/experimental) [I]\n## L007: Provenance storage & citation-first narratives [I]\n## L008: Blast-radius analysis for metric changes [A]\n\n\n# M2: Design Systems & Accessibility ‚Äî Lesson Map - Standards, Tokens & WCAG\n\n## L001: Visual design tokens (color, spacing, typography) for BI [F]\n## L002: Chart component library patterns & theming [I]\n## L003: WCAG 2.2 AA for dashboards (contrast, focus order, keyboard) [I]\n## L004: Color-vision safe palettes & redundancy (shape/texture) [I]\n## L005: Motion, animation, and vestibular safety [I]\n## L006: Print/export safe styles & high-contrast modes [I]\n## L007: Governance: design reviews, linting, and design debt [A]\n## L008: Accessibility QA & assistive tech checks [A]\n\n\n# M3: Perception & Chart Taxonomy ‚Äî Lesson Map - Encodings & Pitfalls\n\n## L001: Encodings: position, length, angle, area, color, texture [F]\n## L002: Chart selection matrix (goal, data type, comparison count) [I]\n## L003: Avoiding lies: axis baselines, area misuse, dual axes risks [I]\n## L004: Uncertainty & error bars, confidence bands, distributions [I]\n## L005: Small multiples vs interaction for comparisons [I]\n## L006: Aggregation bias, Simpson‚Äôs paradox, survivorship bias [A]\n## L007: Sparklines, tilegrams, bullet charts, slope graphs [I]\n## L008: Annotative storytelling & callout patterns [I]\n\n\n# M4: Storytelling & Narrative ‚Äî Lesson Map - What/So-What/Now-What\n\n## L001: Audience mapping & decision context [F]\n## L002: Narrative scaffolds & executive summaries [I]\n## L003: Contrastive explanations & counterfactuals [A]\n## L004: Evidence packing: citations, footnotes, and source links [I]\n## L005: Risk & limitation sections to build trust [I]\n## L006: Talking points generator for presenters [I]\n## L007: Bias/fairness guardrails in messaging [A]\n## L008: Narrative evaluators & rubrics [I]\n\n\n# M5: Data Prep for Visualization ‚Äî Lesson Map - Modeling & Aggregations\n\n## L001: Choosing grains & rollups for charts [F]\n## L002: Dimensional modeling for BI (facts/dimensions, conformed dims) [I]\n## L003: Derived fields & KPI normalization (rates, ratios, per‚Äëcapita) [I]\n## L004: Window functions & percentiles for distributions [I]\n## L005: Time series shaping (calendar tables, week/year variants) [I]\n## L006: Pre-aggregation/extract strategies vs live query [A]\n## L007: Sampling & sketching (hyperloglog, t‚Äëdigest) [A]\n## L008: Quality checks & guardrails before render [I]\n\n\n# M6: Security & Privacy ‚Äî Lesson Map - RLS/CLS, PII & Entitlements\n\n## L001: Row-level & column-level security patterns in BI tools [I]\n## L002: Entitlement-aware queries & token scoping [A]\n## L003: Masking & redaction for screens/exports [I]\n## L004: Consent/purpose limitation in embedded contexts [A]\n## L005: Audit trails for views, exports & shares [I]\n## L006: Secrets & signed URLs for tiles/embeds [I]\n## L007: Pseudonymization & small‚Äën suppression [A]\n## L008: Privacy incident playbooks for BI [A]\n\n\n# M7: Dashboard Engineering ‚Äî Lesson Map - Layouts, States & Interactions\n\n## L001: Layout grids, density, and scan patterns (F/Z) [F]\n## L002: Filter/state management (global vs scoped) [I]\n## L003: Drill-down/drill-through & linked highlighting [I]\n## L004: Parameterization & what-if controls [I]\n## L005: Mobile/responsive/adaptive breakpoints [I]\n## L006: Offline/low‚Äëbandwidth fallbacks & progressive disclosure [A]\n## L007: Internationalization: locale, number, date, currency, bidi [I]\n## L008: Component contracts & reusability [I]\n\n\n# M8: Performance & Scale ‚Äî Lesson Map - Caching, Extracts & Budgets\n\n## L001: Perf budgets for pages, charts, queries [F]\n## L002: Cache hierarchy (client/edge/app/DB/BI extracts) [I]\n## L003: Aggregation tables/materialized views [I]\n## L004: Query tuning (pruning, partition filters, indexes) [A]\n## L005: Incremental refresh & cache invalidation [A]\n## L006: Tile pre-rendering & snapshot scheduling [I]\n## L007: Tail latency analysis & p95/p99 dashboards [I]\n## L008: Cost/perf experiments & SLOs [A]\n\n\n# M9: BI Platforms ‚Äî Lesson Map - Tableau, Power BI, Looker, ThoughtSpot\n\n## L001: Platform selection matrix & constraints [F]\n## L002: Tableau: extracts, LOD, actions, server governance [I]\n## L003: Power BI: models, DAX, gateways, deployment pipelines [I]\n## L004: Looker: LookML, PDTs, explores, caching [I]\n## L005: ThoughtSpot: pinboards, answer search, worksheets [I]\n## L006: Admin/tenant governance & capacity planning [A]\n## L007: Security/embedding per platform [I]\n## L008: Interop & coexistence strategies [A]\n\n\n# M10: Web Visualization Stack ‚Äî Lesson Map - D3, Vega, ECharts, Plotly, WebGL\n\n## L001: Choosing a stack (spec vs API; Canvas vs SVG vs WebGL) [F]\n## L002: Dataflow & state management for custom viz [I]\n## L003: Scale systems, axes, and interaction frameworks [I]\n## L004: High‚Äëdensity rendering with Canvas/WebGL [A]\n## L005: Performance profiling & flamegraphs [A]\n## L006: Accessibility in custom components [I]\n## L007: Testing: unit, screenshot diffs, storybook [I]\n## L008: Packaging, theming, and versioning [I]\n\n\n# M11: Geospatial & Mapping ‚Äî Lesson Map - Tiles, Projections & Layers\n\n## L001: Projections & distortions; when to use which [F]\n## L002: Vector vs raster tiles; providers & caching [I]\n## L003: Choropleths, hexbin, dot density, flow maps [I]\n## L004: Geocoding, reverse geocoding & privacy [I]\n## L005: Clustering & server‚Äëside binning at scale [A]\n## L006: Map interactions & snapping tolerances [I]\n## L007: Mapbox/Leaflet/Kepler.gl patterns [I]\n## L008: Performance & attribution requirements [I]\n\n\n# M12: Real-Time & Streaming Visualization ‚Äî Lesson Map - Live Signals\n\n## L001: WebSockets/SSE & backpressure in UIs [I]\n## L002: Incremental rendering & windowing strategies [A]\n## L003: Event ordering & out‚Äëof‚Äëorder handling visually [A]\n## L004: Alert thresholds & sparklines for tails [I]\n## L005: Streaming aggregates & watermarking [A]\n## L006: Rate‚Äëlimit handling & fallbacks [A]\n## L007: Incident dashboards & noise controls [I]\n## L008: Evidence: link charts to log/trace exemplars [I]\n\n\n# M13: Interactive Analytics ‚Äî Lesson Map - Crossfilter, LOD & Ad‚ÄëHoc\n\n## L001: Crossfilter principles & brushing/linking [I]\n## L002: Level‚Äëof‚Äëdetail controls and pitfalls [I]\n## L003: Ad‚Äëhoc builders with guardrails [I]\n## L004: Session state & undo/redo history [I]\n## L005: Query shaping & safe limits for ad‚Äëhoc [A]\n#\n\n[Content structure truncated for token limits. Focus on generating the lesson based on the template structure.]\n\n---\n\nYou are generating a polished, publication-ready lesson for an AI-Native SaaS Curriculum.\nRead and internalize these two inputs before writing anything:\n\n1. LESSON_GENERATION_PROMPT_GENERIC.md ‚Äî this defines the canonical 10-section structure.\n2. content_structure_ai-native-saas-curriculum-lesson-maps.md ‚Äî this defines where the lesson fits in the broader curriculum.\n\n---\n\n### LESSON METADATA\n- Lesson Code: M24-L008\n- Lesson Title: Executive narrative: what/so‚Äëwhat/now‚Äëwhat & outcomes\n- Module: M24 ‚Äî Capstone\n- Subtopic / Focus Area: General\n- Complexity Level: I (Intermediate)\n- Estimated Duration: 60 minutes\n- Target Audience: Technical professionals and developers\n- Organization Context: Organizations implementing solutions\n\n**Prerequisites & Context**\n- Prior Knowledge Required: Basic understanding of the domain\n- Related Lessons: Related lessons in the same module\n- Domain Context: Technology and digital transformation\n\n---\n\n### INSTRUCTIONS\n\nYou must generate a **complete, polished lesson** following the exact 10-section structure of the template.\nEach section must be written in clean, engaging, professional prose using proper Markdown syntax.\n\n**Length & Depth Guidelines**\n- [F] Foundation: ~350‚Äì400 lines, 2‚Äì3 core concepts, 3 scenarios  \n- [I] Intermediate: ~400‚Äì500 lines, 3‚Äì4 core concepts, 4 scenarios  \n- [A] Advanced: 500+ lines, 4+ concepts, multiple scenarios  \n\n**Content & Technical Requirements**\n1. Include working code examples (‚â•15 lines each) demonstrating realistic SaaS architecture patterns.  \n   Languages accepted: TypeScript/Node.js, SQL, Terraform, or YAML.  \n2. Use real metrics and trade-offs (e.g., ‚Äúreduces provisioning time by 60%‚Äù, ‚Äúsaves $50K annually‚Äù).  \n3. Anchor explanations in SaaS-specific terminology: multi-tenancy, tenant isolation, subscription models, usage-based pricing, CI/CD, and observability.  \n4. Connect code and architecture examples to common SaaS stacks ‚Äî Next.js, Postgres, Vercel, AWS, and Terraform.  \n5. Discuss business impact, not just technology. Every technical concept must map to a measurable outcome (cost, velocity, reliability, compliance).  \n6. Avoid filler phrases (‚Äúlearn about‚Äù, ‚Äúunderstand‚Äù); use precise, action-oriented verbs.  \n7. Each code example must include a short contextual explanation and expected outcome.  \n8. Every section should open with 1‚Äì2 framing sentences that orient the learner before details or lists.  \n9. Use active voice, short sentences (under 25 words), and clear transitions.  \n10. Maintain tone balance: professional, instructive, and confident ‚Äî not academic or conversational.  \n\n---\n\n### FORMATTING & STYLE GUIDELINES\n\n- **Headings:** Use `##` for major sections, `###` for subsections. Capitalize key words.  \n- **Bullets:** Use `-` for list items. Avoid numbered lists unless order matters.  \n- **Code Blocks:** Use fenced code blocks with language tags (` ```ts`, ` ```sql`, etc.) and concise comments.  \n- **Tables:** Use clean Markdown tables with short headers; follow with 1‚Äì2 lines of interpretation guidance.  \n- **Callouts:** Use icons like ‚úÖ, ‚ö†Ô∏è, üí° for emphasis when appropriate.  \n- **Spacing:** Insert blank lines before and after headings, code blocks, and tables for readability.  \n- **Tone & Flow:** Alternate between explanation and application. Every theoretical statement should lead into an example or outcome.  \n- **Lexical Clarity:** When using technical terms (e.g., ‚Äútenancy isolation‚Äù), add a short clarifier in parentheses on first use.  \n\n---\n\n### QUALITY GUARANTEES\n\nYour final output must:\n- Contain all 10 sections from the template in order.  \n- Have consistent structure, formatting, and flow.  \n- Read naturally, as if authored by an experienced SaaS educator.  \n- Contain no meta-text, instructions, or placeholders.  \n- Begin immediately with `# Lesson M24-L008: Executive narrative: what/so‚Äëwhat/now‚Äëwhat & outcomes` or the first section heading.  \n\nDo **not** include:\n- Any ‚ÄúLLM Prompt‚Äù or template boilerplate.  \n- Any context or metadata explanation.  \n- Any quality assurance notes at the end.\n\n---\n\n### FINAL OUTPUT\n\nDeliver the **final, learner-facing Markdown lesson**, cleanly formatted and publication-ready."
          }
        ]
      }
    }
  ]
}