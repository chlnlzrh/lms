#!/usr/bin/env python3
"""
Comprehensive Requirements-Based Testing Suite
Based on all user prompts and requirements from the conversation
Optimized for 10 core/20 thread/32GB RAM machine
"""

import asyncio
import time
import json
import aiohttp
from datetime import datetime
from playwright.async_api import async_playwright
from pathlib import Path

class ComprehensiveRequirementsTester:
    def __init__(self, base_url="http://localhost:3001"):
        self.base_url = base_url
        self.results = {}
        self.start_time = None
        
        # Based on user requirements - 20 modules with 541 total lessons
        self.modules = [f"module-{i}" for i in range(1, 21)]
        self.expected_total_lessons = 541
        
    async def test_requirement_1_platform_structure(self):
        """Test: Complete data engineering platform with 541 lessons across 20 modules"""
        result = {
            "requirement": "Complete data engineering platform with 541 lessons across 20 modules",
            "tests": {},
            "success": False
        }
        
        try:
            playwright = await async_playwright().start()
            browser = await playwright.chromium.launch(headless=True)
            page = await browser.new_page()
            
            await page.goto(f"{self.base_url}/", wait_until="networkidle", timeout=30000)
            
            # Test 1.1: 20 modules present
            module_elements = await page.query_selector_all('[data-testid*="module"], .module-card, [class*="module"]')
            
            # Alternative: Count by content analysis
            page_content = await page.content()
            module_mentions = page_content.lower().count('module')
            
            result["tests"]["module_count"] = {
                "passed": len(module_elements) >= 10 or module_mentions >= 20,
                "found_elements": len(module_elements),
                "content_mentions": module_mentions,
                "expected": 20
            }
            
            # Test 1.2: Lessons count verification (check if 541 lessons mentioned)
            has_lesson_count = "541" in page_content or "lesson" in page_content.lower()
            result["tests"]["lesson_count_reference"] = {
                "passed": has_lesson_count,
                "has_541_reference": "541" in page_content,
                "has_lesson_mentions": "lesson" in page_content.lower()
            }
            
            # Test 1.3: Data engineering context
            de_keywords = ["data engineering", "data warehouse", "etl", "pipeline", "analytics"]
            de_presence = sum(1 for keyword in de_keywords if keyword in page_content.lower())
            result["tests"]["data_engineering_context"] = {
                "passed": de_presence >= 3,
                "keywords_found": de_presence,
                "total_keywords": len(de_keywords)
            }
            
            await browser.close()
            await playwright.stop()
            
        except Exception as e:
            result["error"] = str(e)
            
        result["success"] = all(test.get("passed", False) for test in result["tests"].values())
        return result
    
    async def test_requirement_2_enhanced_module_descriptions(self):
        """Test: Module descriptions integration with dynamic content loading"""
        result = {
            "requirement": "Enhanced module descriptions with dynamic content loading",
            "tests": {},
            "success": False
        }
        
        try:
            playwright = await async_playwright().start()
            browser = await playwright.chromium.launch(headless=True)
            page = await browser.new_page()
            
            await page.goto(f"{self.base_url}/", wait_until="networkidle", timeout=30000)
            
            # Test 2.1: API endpoint availability
            async with aiohttp.ClientSession() as session:
                async with session.get(f"{self.base_url}/api/module-descriptions") as response:
                    api_success = response.status == 200
                    if api_success:
                        api_data = await response.json()
                        has_descriptions = "descriptions" in api_data
                        description_count = len(api_data.get("descriptions", {}))
                    else:
                        has_descriptions = False
                        description_count = 0
            
            result["tests"]["api_endpoint"] = {
                "passed": api_success and has_descriptions,
                "status_code": response.status if 'response' in locals() else None,
                "has_descriptions": has_descriptions,
                "description_count": description_count
            }
            
            # Test 2.2: Enhanced module cards with rich content
            await page.wait_for_timeout(2000)  # Wait for dynamic content
            
            # Look for enhanced content indicators
            enhanced_indicators = await page.query_selector_all('[class*="description"], [class*="enhanced"], [class*="rich"]')
            
            # Check for expand/collapse functionality
            expand_buttons = await page.query_selector_all('button[class*="chevron"], button[class*="expand"], [class*="expandable"]')
            
            result["tests"]["enhanced_content"] = {
                "passed": len(enhanced_indicators) > 0 or len(expand_buttons) > 0,
                "enhanced_elements": len(enhanced_indicators),
                "expand_buttons": len(expand_buttons)
            }
            
            # Test 2.3: Dynamic content loading
            page_content = await page.content()
            dynamic_indicators = [
                "prerequisite" in page_content.lower(),
                "learning objective" in page_content.lower(),
                "skill" in page_content.lower(),
                "hour" in page_content.lower(),
                "description" in page_content.lower()
            ]
            
            result["tests"]["dynamic_content"] = {
                "passed": sum(dynamic_indicators) >= 3,
                "indicators_found": sum(dynamic_indicators),
                "total_indicators": len(dynamic_indicators)
            }
            
            await browser.close()
            await playwright.stop()
            
        except Exception as e:
            result["error"] = str(e)
            
        result["success"] = all(test.get("passed", False) for test in result["tests"].values())
        return result
    
    async def test_requirement_3_performance_optimization(self):
        """Test: Performance optimization for 10 core/20 thread machine"""
        result = {
            "requirement": "Performance optimization for powerful machine",
            "tests": {},
            "success": False
        }
        
        try:
            playwright = await async_playwright().start()
            browser = await playwright.chromium.launch(headless=True)
            page = await browser.new_page()
            
            # Test 3.1: Page load performance
            start_time = time.time()
            await page.goto(f"{self.base_url}/", wait_until="networkidle", timeout=30000)
            load_time = time.time() - start_time
            
            result["tests"]["load_performance"] = {
                "passed": load_time < 10.0,  # Reasonable for development server
                "load_time": round(load_time, 3),
                "threshold": 10.0
            }
            
            # Test 3.2: Concurrent API handling
            concurrent_start = time.time()
            tasks = []
            async with aiohttp.ClientSession() as session:
                for i in range(10):  # Simulate concurrent requests
                    task = session.get(f"{self.base_url}/api/module-descriptions")
                    tasks.append(task)
                
                responses = await asyncio.gather(*tasks, return_exceptions=True)
                concurrent_time = time.time() - concurrent_start
                
                successful_responses = sum(1 for r in responses if hasattr(r, 'status') and r.status == 200)
            
            result["tests"]["concurrent_handling"] = {
                "passed": successful_responses >= 8 and concurrent_time < 5.0,
                "successful_requests": successful_responses,
                "total_requests": 10,
                "concurrent_time": round(concurrent_time, 3)
            }
            
            # Test 3.3: Resource efficiency
            # Check for lazy loading, caching indicators
            page_content = await page.content()
            efficiency_indicators = [
                "loading" in page_content.lower(),
                "cache" in page_content.lower(),
                len(await page.query_selector_all('img[loading="lazy"]')) > 0
            ]
            
            result["tests"]["resource_efficiency"] = {
                "passed": sum(efficiency_indicators) >= 1,
                "efficiency_indicators": sum(efficiency_indicators)
            }
            
            await browser.close()
            await playwright.stop()
            
        except Exception as e:
            result["error"] = str(e)
            
        result["success"] = all(test.get("passed", False) for test in result["tests"].values())
        return result
    
    async def test_requirement_4_ui_ux_enhancements(self):
        """Test: Enhanced UI/UX with progressive disclosure and responsive design"""
        result = {
            "requirement": "Enhanced UI/UX with progressive disclosure and responsive design",
            "tests": {},
            "success": False
        }
        
        try:
            playwright = await async_playwright().start()
            browser = await playwright.chromium.launch(headless=True)
            page = await browser.new_page()
            
            await page.goto(f"{self.base_url}/", wait_until="networkidle", timeout=30000)
            
            # Test 4.1: Progressive disclosure
            expand_elements = await page.query_selector_all('button[class*="chevron"], [class*="expandable"], [class*="collaps"]')
            
            # Try to interact with expandable content
            if expand_elements:
                try:
                    await expand_elements[0].click()
                    await page.wait_for_timeout(500)
                    interaction_successful = True
                except:
                    interaction_successful = False
            else:
                interaction_successful = False
            
            result["tests"]["progressive_disclosure"] = {
                "passed": len(expand_elements) > 0,
                "expandable_elements": len(expand_elements),
                "interaction_test": interaction_successful
            }
            
            # Test 4.2: Responsive design - Desktop
            await page.set_viewport_size({"width": 1920, "height": 1080})
            await page.wait_for_timeout(300)
            desktop_cards = await page.query_selector_all('[class*="card"]')
            
            # Test 4.3: Responsive design - Tablet
            await page.set_viewport_size({"width": 768, "height": 1024})
            await page.wait_for_timeout(300)
            tablet_cards = await page.query_selector_all('[class*="card"]')
            
            # Test 4.4: Responsive design - Mobile
            await page.set_viewport_size({"width": 375, "height": 667})
            await page.wait_for_timeout(300)
            mobile_cards = await page.query_selector_all('[class*="card"]')
            
            result["tests"]["responsive_design"] = {
                "passed": len(desktop_cards) > 0 and len(tablet_cards) > 0 and len(mobile_cards) > 0,
                "desktop_cards": len(desktop_cards),
                "tablet_cards": len(tablet_cards),
                "mobile_cards": len(mobile_cards)
            }
            
            # Test 4.5: Interactive elements
            buttons = await page.query_selector_all('button')
            links = await page.query_selector_all('a')
            interactive_count = len(buttons) + len(links)
            
            result["tests"]["interactive_elements"] = {
                "passed": interactive_count >= 10,
                "buttons": len(buttons),
                "links": len(links),
                "total_interactive": interactive_count
            }
            
            await browser.close()
            await playwright.stop()
            
        except Exception as e:
            result["error"] = str(e)
            
        result["success"] = all(test.get("passed", False) for test in result["tests"].values())
        return result
    
    async def test_requirement_5_github_integration(self):
        """Test: GitHub deployment and version control integration"""
        result = {
            "requirement": "GitHub deployment and version control integration",
            "tests": {},
            "success": False
        }
        
        try:
            # Test 5.1: Check if running from git repository
            git_dir = Path(".git")
            has_git = git_dir.exists()
            
            # Test 5.2: Check for GitHub-related files
            github_files = [
                Path(".github").exists(),
                Path("README.md").exists(),
                Path("package.json").exists()
            ]
            
            result["tests"]["repository_structure"] = {
                "passed": has_git and sum(github_files) >= 2,
                "has_git": has_git,
                "github_files_present": sum(github_files)
            }
            
            # Test 5.3: Deployment readiness
            playwright = await async_playwright().start()
            browser = await playwright.chromium.launch(headless=True)
            page = await browser.new_page()
            
            await page.goto(f"{self.base_url}/", wait_until="networkidle", timeout=30000)
            
            # Check for production-ready indicators
            page_content = await page.content()
            prod_indicators = [
                "<!DOCTYPE html>" in page_content,
                "next" in page_content.lower(),
                not "localhost" in await page.title(),  # Should work for production
                len(await page.query_selector_all('script')) > 0
            ]
            
            result["tests"]["deployment_readiness"] = {
                "passed": sum(prod_indicators) >= 3,
                "production_indicators": sum(prod_indicators)
            }
            
            await browser.close()
            await playwright.stop()
            
        except Exception as e:
            result["error"] = str(e)
            
        result["success"] = all(test.get("passed", False) for test in result["tests"].values())
        return result
    
    async def run_comprehensive_tests(self):
        """Run all requirement-based tests in parallel"""
        print("COMPREHENSIVE REQUIREMENTS-BASED TESTING")
        print("=" * 60)
        print(f"Target: {self.base_url}")
        print(f"Machine: 10 core/20 thread/32GB RAM optimized")
        print(f"Requirements: 5 major categories")
        print("-" * 60)
        
        self.start_time = time.time()
        
        # Execute all requirement tests in parallel
        tasks = [
            self.test_requirement_1_platform_structure(),
            self.test_requirement_2_enhanced_module_descriptions(),
            self.test_requirement_3_performance_optimization(),
            self.test_requirement_4_ui_ux_enhancements(),
            self.test_requirement_5_github_integration()
        ]
        
        print(f"Executing {len(tasks)} requirement test categories in parallel...")
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Process results
        requirement_names = [
            "Platform Structure",
            "Module Descriptions",
            "Performance Optimization", 
            "UI/UX Enhancements",
            "GitHub Integration"
        ]
        
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                self.results[requirement_names[i]] = {
                    "error": str(result),
                    "success": False
                }
            else:
                self.results[requirement_names[i]] = result
        
        total_time = time.time() - self.start_time
        print(f"Completed in {total_time:.2f} seconds")
        
        return self.results
    
    def generate_comprehensive_report(self):
        """Generate detailed requirements compliance report"""
        total_requirements = len(self.results)
        successful_requirements = sum(1 for r in self.results.values() if r.get("success", False))
        
        print("\n" + "=" * 80)
        print("COMPREHENSIVE REQUIREMENTS COMPLIANCE REPORT")
        print("=" * 80)
        
        # Overall summary
        compliance_rate = (successful_requirements / total_requirements) * 100
        print(f"\nOVERALL COMPLIANCE:")
        print(f"  Total Requirements Tested: {total_requirements}")
        print(f"  Successfully Met: {successful_requirements}")
        print(f"  Failed/Partial: {total_requirements - successful_requirements}")
        print(f"  Compliance Rate: {compliance_rate:.1f}%")
        
        # Detailed requirement analysis
        print(f"\nDETAILED REQUIREMENT ANALYSIS:")
        for req_name, result in self.results.items():
            if result.get("success", False):
                print(f"  ‚úÖ {req_name}: COMPLIANT")
            else:
                print(f"  ‚ùå {req_name}: NON-COMPLIANT")
            
            if "tests" in result:
                for test_name, test_result in result["tests"].items():
                    status = "PASS" if test_result.get("passed", False) else "FAIL"
                    print(f"     - {test_name.replace('_', ' ').title()}: {status}")
        
        # Performance metrics
        if self.start_time:
            execution_time = time.time() - self.start_time
            print(f"\nPERFORMANCE METRICS:")
            print(f"  Test Execution Time: {execution_time:.2f}s")
            print(f"  Parallel Efficiency: {total_requirements} requirements tested concurrently")
            print(f"  Machine Utilization: Optimized for 10 core/20 thread")
        
        # Recommendations
        print(f"\nRECOMMENDATIONS:")
        failed_reqs = [name for name, result in self.results.items() if not result.get("success", False)]
        if not failed_reqs:
            print("  üéâ All requirements met! Platform is production-ready.")
        else:
            print(f"  ‚ö†Ô∏è  Address the following requirements:")
            for req in failed_reqs:
                print(f"     - {req}")
        
        # Save detailed report
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"comprehensive_requirements_report_{timestamp}.json"
        with open(report_file, 'w') as f:
            json.dump(self.results, f, indent=2, default=str)
        
        print(f"\nDetailed report saved: {report_file}")
        
        return compliance_rate >= 80.0

async def main():
    """Main execution for comprehensive requirements testing"""
    tester = ComprehensiveRequirementsTester()
    
    try:
        # Run comprehensive tests
        await tester.run_comprehensive_tests()
        
        # Generate detailed report
        success = tester.generate_comprehensive_report()
        
        if success:
            print("\nüéâ SUCCESS: Platform meets all major requirements!")
            return 0
        else:
            print("\n‚ö†Ô∏è  WARNING: Some requirements need attention")
            return 1
            
    except Exception as e:
        print(f"\nüí• ERROR: Comprehensive testing failed - {str(e)}")
        return 1

if __name__ == "__main__":
    import sys
    result = asyncio.run(main())
    sys.exit(result)